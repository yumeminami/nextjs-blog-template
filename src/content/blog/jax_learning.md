---
title: "JAX Learning"
date: "2025-04-25"
draft: false
keywords: ["robotics", "Python", "JAX"]
summary: "JAX å­¦ä¹ ç¬”è®°ï¼Œ å­¦ä¹ ä½¿ç”¨ JAX æ ¸å¿ƒç‰¹æ€§"
featured: true
---

## JAX æ˜¯ä»€ä¹ˆ

JAX æ˜¯ä¸€ä¸ªç”¨äºé«˜æ€§èƒ½ç§‘å­¦è®¡ç®—çš„ Python åº“ã€‚å®ƒæä¾›äº†è‡ªåŠ¨å¾®åˆ†ã€å¹¶è¡Œè®¡ç®—å’Œåˆ†å¸ƒå¼è®¡ç®—ç­‰åŠŸèƒ½ï¼Œé€‚ç”¨äºæœºå™¨å­¦ä¹ ã€ä¼˜åŒ–å’Œç§‘å­¦è®¡ç®—ç­‰é¢†åŸŸã€‚

## JAX çš„ç‰¹ç‚¹

- **grad** è‡ªåŠ¨å¾®åˆ†
- **jit** å³æ—¶ç¼–è¯‘
- **pmap** åˆ†å¸ƒå¼è®¡ç®—
- **vmap** å‘é‡åŒ–è®¡ç®—
- **lax** ç”¨äºæ„å»ºå’Œä¼˜åŒ–å¤æ‚çš„æ•°å­¦è¡¨è¾¾å¼
- **stax** ç”¨äºæ„å»ºå’Œä¼˜åŒ–ç¥ç»ç½‘ç»œ

å°†ä»‹ç» JAX çš„å„ä¸ªç‰¹æ€§ï¼Œ å¹¶ç»™å‡ºç¤ºä¾‹ä»£ç (**ä¸ä¼šè¯¦ç»†ä»‹ç»åŸç†å®ç°**ï¼Œ è€Œæ˜¯ä»ä½¿ç”¨è€…çš„è§’åº¦å‡ºå‘)

## ğŸ” å‡½æ•°å¼è‡ªåŠ¨å¾®åˆ†ï¼ˆAutogradï¼‰

`jax.grad(fun)` æ˜¯ JAX æä¾›çš„ä¸€ä¸ªæ ¸å¿ƒå‡½æ•°è½¬æ¢ï¼ˆfunction transformationï¼‰ï¼Œå®ƒçš„ä½œç”¨æ˜¯ï¼š

1. æ¥æ”¶ä¸€ä¸ª Python å‡½æ•° fun ä½œä¸ºè¾“å…¥ã€‚
2. è¿”å›ä¸€ä¸ªæ–°çš„å‡½æ•°ã€‚
3. å½“ä½ è°ƒç”¨è¿™ä¸ªæ–°çš„å‡½æ•°æ—¶ï¼Œå®ƒä¼šè®¡ç®—å¹¶è¿”å›ï¼š
    - åŸå§‹å‡½æ•° fun åœ¨ç»™å®šè¾“å…¥ä¸Šçš„è¾“å‡ºå€¼ã€‚
    - åŸå§‹å‡½æ•° fun åœ¨ç»™å®šè¾“å…¥ä¸Šå…³äºæŒ‡å®šå‚æ•°çš„æ¢¯åº¦ã€‚


**grad çš„å‚æ•°ï¼š**

- `fun`: è¦è®¡ç®—å…¶å€¼å’Œæ¢¯åº¦çš„å‡½æ•°ã€‚å¯¹äº grad æ“ä½œï¼Œè¿™ä¸ªå‡½æ•°çš„ä¸»è¦è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªæ ‡é‡ï¼ˆæˆ–è€…ä¸€ä¸ªç»“æ„ï¼Œä½ å¯ä»¥æŒ‡å®šå¯¹å…¶æŸä¸ªæ ‡é‡å¶å­è¿›è¡Œå¾®åˆ†ï¼‰ã€‚
- `argnums`: æŒ‡å®šå¯¹ `fun` çš„å“ªä¸ªæˆ–å“ªäº›è¾“å…¥å‚æ•°è®¡ç®—æ¢¯åº¦ã€‚
  - é»˜è®¤ä¸º 0ï¼Œå³å¯¹ç¬¬ä¸€ä¸ªå‚æ•°è®¡ç®—æ¢¯åº¦ã€‚
  - å¯ä»¥æ˜¯ä¸€ä¸ªæ•´æ•°ï¼ˆå¯¹è¯¥ç´¢å¼•å¯¹åº”çš„å‚æ•°ï¼‰ã€‚
  - å¯ä»¥æ˜¯ä¸€ä¸ªæ•´æ•°çš„å…ƒç»„æˆ–åˆ—è¡¨ï¼ˆå¯¹å¤šä¸ªå‚æ•°ï¼‰ã€‚
  - å¯ä»¥æ˜¯ä¸€ä¸ª `pytree` ç»“æ„ï¼Œå…¶ä¸­åŒ…å«ä½ æƒ³è®¡ç®—æ¢¯åº¦çš„å‚æ•°çš„ç´¢å¼•ã€‚


**grad çš„è¿”å›å€¼ï¼š**

- è¿”å›ä¸€ä¸ªå…ƒç»„ (grad)ï¼Œå…¶ä¸­ï¼š
  - grad æ˜¯ fun åœ¨ç»™å®šè¾“å…¥ä¸Šå…³äºæŒ‡å®šå‚æ•°çš„æ¢¯åº¦ã€‚

**ç¤ºä¾‹ï¼š**

```python
import jax
import jax.numpy as jnp
import time

# Define loss function
def loss_fn(w, b, x, y):
    y_pred = w * x + b
    loss = jnp.mean((y_pred - y) ** 2)
    return loss

key = jax.random.PRNGKey(42)

# Generate data
w = jax.random.normal(key, (1000,))
b = jax.random.normal(key, (1000,))
x = jax.random.normal(key, (1000,))

y_true = w * x + b
noise = jax.random.normal(key, (1000,)) * 0.1  # Adding some noise
y = y_true + noise

# use value_and_grad to get loss and gradient
# argnums=(0, 1) means compute gradient for w and b
# grad_fn = jax.value_and_grad(loss_fn, argnums=(0, 1), has_aux=True)
grad_fn = jax.grad(loss_fn, argnums=(0, 1))

# Compute loss and gradient
# loss, grad = grad_fn(w, b, x, y)
grad = grad_fn(w, b, x, y)

# print(loss)
print(grad)

```

- define a loss function, `loss_fn`, it returns loss
- create `grad_fn`, the `argnums=(0, 1)` means the first and second arguments are the ones to be differentiated(`w` and `b` in this case)
- call the `grad_fn`, `loss_fn` will forward compute the `loss` and `grad_fn` will BackPropagation compute the gradient `grad`. finally, it returns a tuple of (loss, grad)

## âš¡  Just-In-Time ç¼–è¯‘ï¼ˆJITï¼‰åŠ é€Ÿ

`jax.jit` æ˜¯ä¸€ä¸ªå‡½æ•°è½¬æ¢å™¨ï¼Œå¯ä»¥å°† Python/JAX ä»£ç ç¼–è¯‘æˆä¼˜åŒ–çš„ã€åœ¨åŠ é€Ÿå™¨ä¸Šé«˜æ•ˆè¿è¡Œçš„æœºå™¨ç ã€‚

è¢« `jax.jit` è£…é¥°çš„å‡½æ•°éœ€è¦éµå¾ªä¸€äº›è§„åˆ™ï¼Œæœ€ä¸»è¦çš„æ˜¯å‡½æ•°è¾“å…¥å’Œå†…éƒ¨æ§åˆ¶æµçš„å½¢çŠ¶å¿…é¡»æ˜¯é™æ€çš„ï¼ˆåœ¨ç¼–è¯‘æ—¶ç¡®å®šï¼‰ã€‚ä¾èµ–äºè¿è¡Œæ—¶å€¼çš„åŠ¨æ€æ§åˆ¶æµéœ€è¦ä½¿ç”¨ `jax.lax` æä¾›çš„åŸè¯­ï¼ˆå¦‚ `jax.lax.cond`, `jax.lax.while_loop`, `jax.lax.scan`ï¼‰ã€‚

å¹¶ä¸”æ‰€æœ‰è¢«è£…é¥°ä¸º `jax.jit` çš„å‡½æ•°ï¼Œ å‡½æ•°ä¸­çš„æ‰€æœ‰ `print` æˆ–è€… `log` è¯­å¥éƒ½ä¸ä¼šæ‰§è¡Œï¼Œ è€Œæ˜¯ä¼šåœ¨ç¼–è¯‘æ—¶è¢«å¿½ç•¥ã€‚
å–è€Œä»£ä¹‹åˆ™ä½¿ç”¨ ` jax.debug.print` æ¥æ‰“å°ä¿¡æ¯ã€‚

ğŸ”’ æ³¨æ„äº‹é¡¹

|é™åˆ¶|æè¿°|
|---|---|
| not support f-string | ä¸æ”¯æŒ `"{} {}".format(x, y)`  |
| not keep order | ä¸èƒ½ä¿è¯æ‰§è¡Œé¡ºåº |



**ç¤ºä¾‹ï¼š**

```python

import jax
import jax.numpy as jnp
import time


def multiply(x, y):
    return x * y


@jax.jit
def multiply_jit(x, y):
    return x * y

size = 1e7
x = jnp.ones(int(size))
y = jnp.ones(int(size))

print("---------No JIT---------")
start = time.time()
multiply(x, y)
end = time.time()
print(f"Time taken: {end - start} seconds")

print("---------JIT(include compile time)---------")
start = time.time()
res = multiply_jit(x, y)
res.block_until_ready()
end = time.time()
print(f"Time taken: {end - start} seconds")


print("---------JIT---------")
start = time.time()
res = multiply_jit(x, y)
res.block_until_ready()
end = time.time()
print(f"Time taken: {end - start} seconds")


# ---------No JIT---------
# Time taken: 0.02260136604309082 seconds
# ---------JIT(include compile time)---------
# Time taken: 0.022781848907470703 seconds
# ---------JIT---------
# Time taken: 0.002478361129760742 seconds
```

## ğŸ”„ å‘é‡åŒ–å‡½æ•°ï¼ˆvmapï¼‰

ä½¿ç”¨ `jax.vmap` è‡ªåŠ¨å°†æ ‡é‡å‡½æ•°å‘é‡åŒ–ï¼Œæ— éœ€å†™ `for-loop`ã€‚

**ç¤ºä¾‹ï¼š**

```python

import jax
import jax.numpy as jnp

def f(x):
    return x + 1

# No vmap
xs = jnp.array([1.0, 2.0, 3.0])
ys = jnp.array([f(x) for x in xs])

# vmap
vmap_f = jax.vmap(f)
ys = vmap_f(xs)

# multi-dimensional vmap
def f(x, y):
    return x + y
 
xs = jnp.array([1.0, 2.0, 3.0])
ys = jnp.array([1.0, 2.0, 3.0])
vmap_f = jax.vmap(f)
ys = vmap_f(xs, ys) # output: [2.0, 4.0, 6.0]
# or
ys = vmap_f(xs, 1) # output: [2.0, 3.0, 4.0]



# vmap with jit
@jax.jit
def loss_fn(w, x, y):
    pred = jnp.dot(w, x)
    return (pred - y) ** 2, 1

x_batch = jnp.array([[1,2,3], [4,5,6], [7,8,9]])  # shape = (3, 3)
y_batch = jnp.array([1.0, 2.0, 3.0])              # shape = (3,)
w = jnp.array([0.1, 0.2, 0.3])                    # shape = (3,)
grad_fn = jax.value_and_grad(loss_fn, argnums=(0,), has_aux=True)

(loss, _), grads = jax.vmap(grad_fn, in_axes=(None, 0, 0))(w, x_batch, y_batch)
print(loss)
print(grads)

```

è¯¦ç»†ä»‹ç»ä¸€ä¸‹ç¤ºä¾‹ä»£ç ä¸­æœ€åä¸€ä¸ª `vmap` çš„ç”¨æ³•ï¼š

- `jax.vmap(grad_fn, in_axes=(None, 0, 0))` å°† `grad_fn` å‘é‡åŒ–ï¼Œ`in_axes=(None, 0, 0)` è¡¨ç¤º `grad_fn` çš„ç¬¬ä¸€ä¸ªå‚æ•°ä¸éœ€è¦å‘é‡åŒ–ï¼Œç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªå‚æ•°éœ€è¦å‘é‡åŒ–ã€‚
- ç›¸å½“äºæ¯æ¬¡è°ƒç”¨ `grad_fn` æ—¶ï¼Œè¾“å…¥çš„ `w` ä¸å˜ï¼Œ`x_batch[i]` å’Œ `y_batch[i]` åˆ†åˆ«ä½œä¸º `grad_fn` çš„ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªå‚æ•°ã€‚


## ğŸ’¡ lax æ ¸å¿ƒç®—å­åº“

`lax` æ˜¯ JAX æä¾›çš„ä¸€ä¸ªæ ¸å¿ƒç®—å­åº“ï¼Œç”¨äºæ„å»ºå’Œä¼˜åŒ–å¤æ‚çš„æ•°å­¦è¡¨è¾¾å¼ã€‚å®ƒæä¾›äº†è®¸å¤šå¸¸ç”¨çš„æ•°å­¦è¿ç®—ï¼Œå¦‚çŸ©é˜µä¹˜æ³•ã€å·ç§¯ã€å½’ä¸€åŒ–ç­‰ã€‚

â€¢	æ„å»ºæ›´åº•å±‚ã€å¯å¾®åˆ†çš„æ§åˆ¶æµï¼ˆæ›¿ä»£ Python åŸç”Ÿçš„ ifã€forï¼‰
â€¢	å®ç°é«˜æ€§èƒ½çš„ XLA åŸè¯­ï¼ˆè·¨ CPUã€GPUã€TPU éƒ½é«˜æ•ˆï¼‰
â€¢	åœ¨ jitã€gradã€vmap ç­‰ç»„åˆä¸­æ›´é«˜æ•ˆç¨³å®š

ä»‹ç»ä¸€äº›å¸¸ç”¨çš„ `lax` ç®—å­ï¼š

- `lax.cond`ï¼šæ¡ä»¶åˆ†æ”¯
- `lax.scan`ï¼šæ‰«æ
- `lax.select/lax.switch`ï¼šåˆ†æ”¯é€‰æ‹©

ğŸš€ 1. æ¡ä»¶æ§åˆ¶æµï¼š`lax.cond`

```python

import jax
from jax import lax
import jax.numpy as jnp

def f(x):
    return lax.cond(x > 0, 
                    lambda x: x**2,    # True åˆ†æ”¯
                    lambda x: -x,      # False åˆ†æ”¯
                    x)

# æµ‹è¯•
print(f(3.0))  # 9.0
print(f(-2.0))  # -2.0

```

- é€‚åˆæ›¿ä»£ Python ä¸­çš„ if/elseï¼Œä¿è¯ JIT ç¼–è¯‘æ—¶é«˜æ•ˆã€‚
- åªæ”¯æŒäºŒå…ƒæ¡ä»¶åˆ†æ”¯ï¼Œ ä¸æ”¯æŒå¤šå…ƒæ¡ä»¶åˆ†æ”¯ã€‚

ğŸš€ 2. æ‰«æï¼š`lax.scan`

`lax.scan` æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¾ªç¯æ“ä½œ, é«˜æ•ˆåœ°åœ¨ GPU æˆ– TPU ä¸Šæ‰§è¡Œã€‚


```python

import jax
from jax import lax
import jax.numpy as jnp

def step(carry, x):
    return carry + x, carry + x

xs = jnp.arange(5)
carry = 0.0

carry, ys = lax.scan(step, carry, xs)

# carry = 10.0
# ys = [0.  1.  3.  6. 10.]

```

- `lax.scan` æ¥å—ä¸€ä¸ªå‡½æ•°ï¼Œä»¥åŠå‡½æ•°éœ€è¦çš„å‚æ•°ï¼Œä»¥åŠåˆå§‹å€¼ `carry`ï¼Œä»¥åŠè¾“å…¥åºåˆ— `xs`ã€‚
- å¾ªç¯æ¬¡æ•°ä¼šè‡ªåŠ¨æ ¹æ® `xs` çš„é•¿åº¦ç¡®å®š, å¦‚æœ `xs` ä¸å­˜åœ¨ï¼Œ ä¹Ÿå¯ä»¥å¯ä»¥æ‰‹åŠ¨æŒ‡å®šï¼Œä¾‹å¦‚ `lax.scan(step, carry, None, length=10)`ã€‚
- æ¯æ¬¡å¾ªç¯ä¸­ï¼Œ `carry` éƒ½ä¼šè¢«æ›´æ–°ï¼Œ å¹¶ä½œä¸ºä¸‹ä¸€æ¬¡å¾ªç¯çš„è¾“å…¥ã€‚`xs` åˆ™ä¼šæ ¹æ®å½“å‰çš„å¾ªç¯æ¬¡æ•° `xs[i]` ä¼ å…¥ `step` å‡½æ•°ã€‚
- `lax.scan` ä¼šä¿æŒä¸€ä¸ªæ›´æ–°çš„çŠ¶æ€ï¼ˆ`carry`ï¼‰ï¼Œè¿”å›æœ€ç»ˆç»“æœå’Œæ‰€æœ‰ä¸­é—´ç»“æœ `ys`ã€‚


ğŸš€ 3. é€‰æ‹©ï¼š`lax.select` å’Œ `lax.switch`

**lax.select**

```python

import jax
from jax import lax
import jax.numpy as jnp

x = jnp.array([1.0, 2.0, 3.0])
y = jnp.array([4.0, 5.0, 6.0])
cond = jnp.array([True, False, True])

result = lax.select(cond, x, y)
print(result)  # [1.0, 5.0, 3.0]

``` 

- æ ¹æ® `cond` çš„å€¼ï¼Œ é€‰æ‹© `x` æˆ– `y` çš„å€¼ã€‚é€‚ç”¨äºéœ€è¦è¿›è¡Œå…ƒç´ çº§é€‰æ‹©çš„åœºåˆã€‚
- æ³¨æ„ `cond` çš„å½¢çŠ¶éœ€è¦å’Œ `x` å’Œ `y` çš„å½¢çŠ¶ä¸€è‡´ã€‚


**lax.switch**

```python

import jax
from jax import lax
import jax.numpy as jnp

x = jnp.array([1, 2, 3])

def case_fn_0(x): return x * 2
def case_fn_1(x): return x + 5
def case_fn_2(x): return x - 1

result = lax.switch(x[0], [case_fn_0, case_fn_1, case_fn_2])
print(result)  # 3 (å› ä¸º 1 å±äº case_fn_0, 1 * 2 = 3)
```

- æ ¹æ® `x[0]` çš„å€¼ï¼Œ é€‰æ‹© `case_fn_0`, `case_fn_1`, `case_fn_2` ä¸­çš„ä¸€ä¸ªå‡½æ•°æ‰§è¡Œã€‚
- é€‚ç”¨äºéœ€è¦æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒå‡½æ•°æ‰§è¡Œçš„åœºåˆã€‚
- æ³¨æ„ `x[0]` çš„å€¼éœ€è¦æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œ ä¸”åœ¨ `[0, n)` ä¹‹é—´ï¼Œ å…¶ä¸­ `n` æ˜¯ `case_fn` çš„æ•°é‡ã€‚
